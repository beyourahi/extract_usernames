<?xml version="1.0" encoding="UTF-8"?>
<task_specification>
  <context>
    <repository>
      <name>extract_usernames</name>
      <url>https://github.com/beyourahi/extract_usernames</url>
      <type>single-file Python CLI tool</type>
      <architecture>standalone script with no framework dependencies</architecture>
      <purpose>Extract Instagram usernames from screenshots using multi-pass OCR with optional VLM consensus</purpose>
    </repository>
    
    <current_vlm_model>
      <name>glm-ocr:bf16</name>
      <size>~2.2GB</size>
      <parameters>0.9B parameters</parameters>
      <deployment>Ollama server (local)</deployment>
      <role>secondary verification and rescue mechanism for failed EasyOCR extractions</role>
      <specialization>Purpose-built OCR model optimized for document understanding and text extraction</specialization>
      <performance>94.62 on OmniDocBench V1.5 (top rank), 99.9% precision mode available</performance>
    </current_vlm_model>
    
    <execution_environment>
      <location>local repository</location>
      <mode>direct execution (not automated workflow)</mode>
      <hardware_support>CUDA (NVIDIA), ROCm (AMD), MPS (Apple Silicon), CPU fallback</hardware_support>
    </execution_environment>

    <current_implementation>
      <ocr_engine>
        <primary>EasyOCR with multi-pass preprocessing</primary>
        <preprocessing_variants>
          <variant name="balanced">CLAHE enhancement + bilateral filter + adaptive threshold</variant>
          <variant name="aggressive">Strong CLAHE + Otsu threshold + morphological closing</variant>
          <variant name="minimal">Fast NL means denoising + adaptive threshold</variant>
        </preprocessing_variants>
        <consensus_logic>Weighted voting (aggressive variant gets 2x weight), requires 3+ votes for consensus</consensus_logic>
      </ocr_engine>

      <vlm_integration>
        <activation_conditions>
          <condition>EasyOCR produces no username across all variants (rescue mode)</condition>
          <condition>User enables VLM via flag (default: enabled, can disable with --no-vlm)</condition>
        </activation_conditions>
        <consensus_behavior>
          <scenario>VLM agrees with EasyOCR: boost confidence to 90%+</scenario>
          <scenario>VLM disagrees with edit distance ‚â§2: prefer longer username, boost confidence to 85%</scenario>
          <scenario>VLM disagrees with edit distance >2: override with VLM result, set confidence to 85%</scenario>
          <scenario>VLM fails: use EasyOCR result only</scenario>
        </consensus_behavior>
      </vlm_integration>

      <advanced_corrections>
        <dot_reconciliation>Cross-variant detection of dots vs 'o' misreads (e.g., user.name vs username)</dot_reconciliation>
        <confusion_corrections>Known OCR pattern fixes (tf‚Üíff, a‚Üí4, x‚Üíd, rn‚Üím, vv‚Üíw, ii‚Üíu, l‚Üí1, 0‚Üío)</confusion_corrections>
        <segment_concatenation>Attempts to merge split username segments (handles EasyOCR text splitting)</segment_concatenation>
      </advanced_corrections>

      <quality_adjustments>
        <image_quality_calculation>Laplacian variance (sharpness) + std dev (contrast) + brightness normalization</image_quality_calculation>
        <confidence_adjustment>Lower confidence for poor quality images (quality score <0.5 reduces confidence)</confidence_adjustment>
      </quality_adjustments>

      <output_structure>
        <verified_file>verified_usernames.md - Auto-verified usernames (confidence ‚â•80%) with HIGH/MED tier labels</verified_file>
        <review_file>needs_review.md - Low confidence, near-duplicates, or failed extractions requiring manual review</review_file>
        <report_file>extraction_report.md - Performance metrics, hardware info, pipeline statistics</report_file>
        <diagnostics>Optional JSON output with per-image variant details (--diagnostics flag)</diagnostics>
      </output_structure>

      <duplicate_handling>
        <exact_duplicates>Skipped entirely, marked with ‚è≠Ô∏è icon during processing</exact_duplicates>
        <near_duplicates>Levenshtein distance ‚â§2, flagged for review with üîç icon</near_duplicates>
        <cross_run_deduplication>Loads existing usernames from verified/review files before processing</cross_run_deduplication>
      </duplicate_handling>

      <performance_optimization>
        <multiprocessing>Uses optimal worker count (min(6, cpu_count-1)) for parallel image processing</multiprocessing>
        <gpu_acceleration>Automatic hardware detection, uses CUDA/ROCm/MPS when available</gpu_acceleration>
        <vlm_worker_throttling>Reduces worker count to 2 when VLM enabled (prevents VRAM exhaustion)</vlm_worker_throttling>
      </performance_optimization>

      <crop_configuration>
        <top_offset>165 pixels from top edge</top_offset>
        <crop_height>90 pixels</crop_height>
        <left_margin>100 pixels</left_margin>
        <right_margin>100 pixels</right_margin>
        <note>Optimized for standard Instagram username placement in screenshots</note>
      </crop_configuration>
    </current_implementation>
  </context>

  <objectives>
    <primary_goals>
      <goal id="1">Maintain and improve OCR accuracy for Instagram username extraction from screenshots</goal>
      <goal id="2">Ensure cross-platform compatibility (Windows, macOS, Linux) with automatic hardware optimization</goal>
      <goal id="3">Provide comprehensive documentation suitable for non-technical users</goal>
      <goal id="4">Optimize VLM model selection for balance between accuracy, speed, and accessibility</goal>
      <goal id="5">Commit and push all changes with appropriate semantic commit messages</goal>
    </primary_goals>
  </objectives>

  <vlm_model_rationale>
    <current_choice>
      <model>glm-ocr:bf16</model>
      <reasons>
        <reason priority="1">
          <title>Purpose-built for OCR tasks</title>
          <explanation>GLM-OCR is specifically fine-tuned for optical character recognition with 94.62 score on OmniDocBench V1.5 (top rank). Unlike general-purpose VLMs, it excels at text extraction which is precisely what username extraction requires.</explanation>
        </reason>
        <reason priority="2">
          <title>Optimal size-performance tradeoff</title>
          <explanation>At 0.9B parameters (~2.2GB), GLM-OCR runs comfortably on consumer hardware (8GB RAM systems) while delivering state-of-the-art OCR accuracy. This is 8x smaller than alternatives like MiniCPM-V 2.6 (8B params) with comparable or better OCR performance.</explanation>
        </reason>
        <reason priority="3">
          <title>Fast inference speed</title>
          <explanation>Lightweight architecture enables 0.67 images/second processing. For a secondary verification layer (VLM only activates on EasyOCR disagreement/failure), speed matters more than marginal accuracy gains from larger models.</explanation>
        </reason>
        <reason priority="4">
          <title>Low VRAM requirements</title>
          <explanation>Requires only ~3-4GB VRAM total including overhead, enabling multiprocessing without GPU memory exhaustion. Allows script to maintain 2 parallel workers even with VLM enabled.</explanation>
        </reason>
        <reason priority="5">
          <title>Preserves special characters</title>
          <explanation>GLM-OCR's holistic text reading accurately captures dots and underscores that EasyOCR tends to split or drop. This addresses the primary weakness in the multi-pass preprocessing pipeline.</explanation>
        </reason>
        <reason priority="6">
          <title>Ollama ecosystem compatibility</title>
          <explanation>First-class Ollama support with stable BF16 quantization, reliable chat API, and straightforward model pulling (ollama pull glm-ocr:bf16).</explanation>
        </reason>
      </reasons>

      <alternatives_considered>
        <model>
          <name>minicpm-v:8b-2.6-q8_0</name>
          <size>~8GB</size>
          <verdict>Too large for secondary verification role. 8x bigger than GLM-OCR with no meaningful accuracy advantage for simple username extraction. Better suited for complex multimodal reasoning tasks.</verdict>
        </model>
        <model>
          <name>qwen2.5vl:7b</name>
          <size>~6GB</size>
          <verdict>General-purpose VLM, not OCR-optimized. 3x larger than GLM-OCR with slower inference. Overkill for username extraction from clean screenshots.</verdict>
        </model>
        <model>
          <name>olmOCR-2-7B-1025</name>
          <size>~7GB</size>
          <verdict>Excellent for complex document OCR (handwriting, tables, formulas) but oversized for Instagram usernames. 3x larger with no benefit for this use case.</verdict>
        </model>
      </alternatives_considered>

      <when_to_upgrade>
        <scenario>Processing degraded images (skew, poor lighting, low resolution) where GLM-OCR struggles</scenario>
        <scenario>Need for multilingual username extraction beyond English alphanumeric</scenario>
        <scenario>Hardware upgrade makes 7B+ models viable (16GB+ RAM, 8GB+ VRAM available)</scenario>
        <scenario>Accuracy requirements exceed 95% auto-verification rate</scenario>
      </when_to_upgrade>

      <when_current_is_sufficient>
        <scenario>Clean mobile screenshots with standard Instagram layout (current use case)</scenario>
        <scenario>Hardware-constrained environments (8GB RAM, integrated graphics)</scenario>
        <scenario>Processing speed is critical (batch processing hundreds of images)</scenario>
        <scenario>Current accuracy meets requirements (typically >85% auto-verified rate)</scenario>
      </when_current_is_sufficient>
    </current_choice>
  </vlm_model_rationale>

  <readme_specifications>
    <target_audience>
      <persona>Non-technical users (marketers, social media managers, lead generators)</persona>
      <persona>Developers unfamiliar with Python ML tooling</persona>
      <persona>Users on Windows/macOS/Linux with varying hardware (integrated graphics to dedicated GPUs)</persona>
    </target_audience>

    <content_structure>
      <section order="1">
        <title>Overview</title>
        <requirements>
          <requirement>One-sentence description: "Extract Instagram usernames from screenshots using advanced OCR and AI vision models"</requirement>
          <requirement>Key features in 3-5 bullet points focusing on user benefits</requirement>
          <requirement>Visual example or sample output snippet (optional)</requirement>
        </requirements>
      </section>

      <section order="2">
        <title>Features</title>
        <requirements>
          <requirement>Multi-pass OCR with 3 preprocessing variants and weighted voting</requirement>
          <requirement>Optional AI vision model (GLM-OCR) for double-checking and rescue</requirement>
          <requirement>Automatic GPU acceleration (NVIDIA, AMD, Apple Silicon, CPU fallback)</requirement>
          <requirement>Intelligent duplicate detection across multiple runs</requirement>
          <requirement>Confidence-based tiers (HIGH ‚â•90%, MED ‚â•80%, REVIEW <80%)</requirement>
          <requirement>Advanced corrections (dot reconciliation, OCR confusion patterns)</requirement>
          <requirement>Cross-platform support (Windows, macOS, Linux)</requirement>
          <requirement>Parallel processing with automatic worker optimization</requirement>
        </requirements>
      </section>

      <section order="3">
        <title>System Requirements</title>
        <requirements>
          <requirement>Minimum: 8GB RAM, 2GB free disk space, Python 3.9+</requirement>
          <requirement>Recommended: 16GB RAM, GPU with 4GB+ VRAM, Python 3.11+</requirement>
          <requirement>OS: Windows 10/11, macOS 11+, Ubuntu 20.04+ (or equivalent Linux)</requirement>
          <requirement>Optional: Ollama for VLM features (can run EasyOCR-only with --no-vlm)</requirement>
        </requirements>
      </section>

      <section order="4">
        <title>Installation</title>
        <requirements>
          <requirement>Separate subsections for Windows, macOS, Linux with collapsible details</requirement>
          <requirement>Step-by-step Python installation (link to python.org, specify 3.9+ requirement)</requirement>
          <requirement>Single pip install command: pip install -r requirements.txt</requirement>
          <requirement>Ollama installation (optional but recommended):
            - Windows: Download from ollama.com/download
            - macOS: brew install ollama
            - Linux: curl -fsSL https://ollama.com/install.sh | sh</requirement>
          <requirement>Model download: ollama pull glm-ocr:bf16</requirement>
          <requirement>Verification step: python extract_usernames.py --help (should show usage without errors)</requirement>
        </requirements>
      </section>

      <section order="5">
        <title>Quick Start</title>
        <requirements>
          <requirement>Example: python extract_usernames.py my_screenshots (assumes folder on Desktop)</requirement>
          <requirement>Show where outputs appear: ~/Desktop/leads/ by default</requirement>
          <requirement>Explain verified_usernames.md (ready to use, copy to spreadsheet/CRM)</requirement>
          <requirement>Explain needs_review.md (manual verification needed, includes reasons)</requirement>
          <requirement>Mention extraction_report.md (performance metrics and summary)</requirement>
        </requirements>
      </section>

      <section order="6">
        <title>Usage</title>
        <requirements>
          <requirement>Basic: python extract_usernames.py [folder_name]</requirement>
          <requirement>Custom output: python extract_usernames.py images --output /path/to/results</requirement>
          <requirement>Diagnostics: python extract_usernames.py images --diagnostics (saves debug images + JSON)</requirement>
          <requirement>EasyOCR-only: python extract_usernames.py images --no-vlm (skip AI vision model)</requirement>
          <requirement>Input requirements: JPG/PNG/WEBP screenshots with Instagram username visible in expected crop region</requirement>
          <requirement>Output file formats with sample snippets showing Markdown structure</requirement>
        </requirements>
      </section>

      <section order="7">
        <title>How It Works</title>
        <requirements>
          <requirement>ASCII pipeline diagram or step-by-step description:
            1. Crop image to username region (165px from top, 90px height)
            2. Run 3 preprocessing variants (balanced/aggressive/minimal)
            3. EasyOCR reads each variant, votes on best username
            4. Apply dot reconciliation and confusion corrections
            5. Optional: Send to GLM-OCR for verification/rescue
            6. Assign confidence tier and route to verified/review file</requirement>
          <requirement>Explain crop region (why screenshots need consistent layout)</requirement>
          <requirement>Explain confidence tiers: HIGH (90%+), MED (80-89%), REVIEW (<80%)</requirement>
          <requirement>Explain when VLM helps (rescues failures, validates uncertain results)</requirement>
        </requirements>
      </section>

      <section order="8">
        <title>Troubleshooting</title>
        <requirements>
          <requirement>Python not found: Check PATH, reinstall from python.org</requirement>
          <requirement>pip install fails: Try pip3, use --user flag, check internet connection</requirement>
          <requirement>Ollama not running: Start with 'ollama serve' or check system service</requirement>
          <requirement>Model not found: Run 'ollama pull glm-ocr:bf16' and wait for download</requirement>
          <requirement>Low accuracy: Check if screenshots match expected crop region, try --diagnostics to see debug images</requirement>
          <requirement>Slow processing: Ensure GPU is detected (check report for device info), reduce worker count if running out of memory</requirement>
          <requirement>EasyOCR-only fallback: Use --no-vlm flag if Ollama installation fails</requirement>
        </requirements>
      </section>

      <section order="9">
        <title>Advanced: VLM Model Alternatives</title>
        <requirements>
          <requirement>Note: glm-ocr:bf16 is the recommended default for this use case</requirement>
          <requirement>Alternative 1: minicpm-v:8b-2.6-q8_0 (larger, general-purpose VLM, 8GB, better for complex images)</requirement>
          <requirement>Alternative 2: qwen2.5vl:7b (general-purpose, 6GB, good all-rounder)</requirement>
          <requirement>To change model: Edit line 38 in extract_usernames.py: VLM_MODEL = 'your-model-name'</requirement>
          <requirement>Download alternative: ollama pull [model-name]</requirement>
          <requirement>Warning: Larger models require more VRAM and reduce parallel processing capability</requirement>
        </requirements>
      </section>

      <section order="10">
        <title>FAQ</title>
        <requirements>
          <requirement>Q: Why do some usernames need review? A: Low confidence (<80%), poor image quality, or near-duplicates flagged for manual verification</requirement>
          <requirement>Q: Can I process non-Instagram screenshots? A: Only if username appears in same crop region (165px from top). Script is optimized for Instagram layout.</requirement>
          <requirement>Q: Does this work offline? A: Yes, after initial downloads (Python packages, Ollama, GLM-OCR model)</requirement>
          <requirement>Q: Is GPU required? A: No, runs on CPU. GPU makes it 5-10x faster.</requirement>
          <requirement>Q: How accurate is it? A: Typically >85% auto-verified on clean screenshots. Quality depends on image resolution and lighting.</requirement>
          <requirement>Q: What about privacy? A: Everything runs locally, no cloud API calls, no data leaves your machine</requirement>
        </requirements>
      </section>

      <section order="11">
        <title>Contributing</title>
        <requirements>
          <requirement>Follow conventional commits (feat:, fix:, docs:, chore:, refactor:)</requirement>
          <requirement>Submit feature requests via GitHub issues</requirement>
          <requirement>Note: Single-file architecture is intentional for simplicity, PRs should maintain this</requirement>
          <requirement>Test on multiple platforms before submitting</requirement>
        </requirements>
      </section>

      <section order="12">
        <title>License</title>
        <requirements>
          <requirement>Specify license (recommend MIT for maximum accessibility)</requirement>
        </requirements>
      </section>
    </content_structure>

    <style_guidelines>
      <guideline>Use clear, jargon-free language ("AI vision model" instead of "VLM" in user sections)</guideline>
      <guideline>Visual hierarchy: headers, bullets, code blocks, blockquotes for important notes</guideline>
      <guideline>Emoji sparingly: ‚úÖ success, ‚ö†Ô∏è warnings, üí° tips, üöÄ performance, üîç review</guideline>
      <guideline>Short paragraphs (2-3 sentences max)</guideline>
      <guideline>Collapsible sections for platform-specific instructions to reduce clutter</guideline>
      <guideline>Test all commands on fresh environment before publishing</guideline>
    </style_guidelines>
  </readme_specifications>

  <requirements_txt_specification>
    <dependencies>
      <dependency>
        <name>easyocr</name>
        <version>>=1.7.0</version>
        <purpose>Primary OCR engine</purpose>
      </dependency>
      <dependency>
        <name>opencv-python</name>
        <version>>=4.8.0,<5.0.0</version>
        <purpose>Image preprocessing and manipulation</purpose>
      </dependency>
      <dependency>
        <name>numpy</name>
        <version>>=1.24.0</version>
        <purpose>Array operations for image processing</purpose>
      </dependency>
      <dependency>
        <name>torch</name>
        <version>>=2.0.0</version>
        <purpose>EasyOCR backend and hardware acceleration</purpose>
      </dependency>
      <dependency>
        <name>torchvision</name>
        <version>>=0.15.0</version>
        <purpose>Vision utilities for EasyOCR</purpose>
      </dependency>
      <dependency>
        <name>ollama</name>
        <version>>=0.1.0</version>
        <purpose>VLM integration (optional, graceful degradation if missing)</purpose>
      </dependency>
    </dependencies>
    
    <notes>
      <note>Pin major versions, allow minor/patch updates for security fixes</note>
      <note>torch/torchvision will auto-detect CUDA version and install appropriate binaries</note>
      <note>Total download size: ~2-3GB (PyTorch is largest component)</note>
    </notes>
  </requirements_txt_specification>

  <cross_platform_setup_scripts>
    <windows_script>
      <filename>setup.ps1</filename>
      <features>
        <feature>Check Python 3.9+ in PATH, prompt to install from python.org if missing</feature>
        <feature>Run pip install -r requirements.txt with progress output</feature>
        <feature>Check if Ollama is installed, offer download link if missing</feature>
        <feature>If Ollama present: ollama pull glm-ocr:bf16 with progress</feature>
        <feature>Validate: python extract_usernames.py --help (check for clean output)</feature>
        <feature>Print success message with next steps</feature>
      </features>
      <error_handling>
        <handling>If Python missing: Display python.org link, exit with code 1</handling>
        <handling>If pip fails: Suggest --user flag or virtual environment</handling>
        <handling>If Ollama missing: Note that --no-vlm flag can be used, continue</handling>
      </error_handling>
    </windows_script>

    <unix_script>
      <filename>setup.sh</filename>
      <compatibility>macOS and Linux (Bash 4+)</compatibility>
      <features>
        <feature>Detect OS (macOS vs Linux) for platform-specific commands</feature>
        <feature>Check Python 3.9+ availability (python3 --version)</feature>
        <feature>macOS: Offer Homebrew install if missing (brew install python@3.11)</feature>
        <feature>Linux: Detect distro, suggest apt/yum/pacman command for Python</feature>
        <feature>Recommend virtual environment: python3 -m venv venv && source venv/bin/activate</feature>
        <feature>pip3 install -r requirements.txt</feature>
        <feature>Ollama check:
          - macOS: brew install ollama
          - Linux: curl -fsSL https://ollama.com/install.sh | sh</feature>
        <feature>ollama pull glm-ocr:bf16</feature>
        <feature>Validate installation</feature>
      </features>
      <permissions>
        <note>Mark executable: chmod +x setup.sh</note>
        <note>May require sudo for Linux system package installation</note>
      </permissions>
    </unix_script>

    <validation_common>
      <step>Run python3 extract_usernames.py --help</step>
      <step>Check exit code (0 = success)</step>
      <step>If failure: Print python3 --version, pip list, ollama list for diagnostics</step>
      <step>Print summary: Python ‚úÖ, Dependencies ‚úÖ, Ollama ‚úÖ/‚ö†Ô∏è, Ready to use</step>
    </validation_common>
  </cross_platform_setup_scripts>

  <commit_strategy>
    <commits>
      <commit order="1">
        <message>docs: add comprehensive README with installation and usage guide</message>
        <files>
          <file>README.md</file>
        </files>
        <notes>Complete user-facing documentation following readme_specifications</notes>
      </commit>

      <commit order="2">
        <message>chore: add requirements.txt for dependency management</message>
        <files>
          <file>requirements.txt</file>
        </files>
        <notes>Pin dependencies with compatible version ranges</notes>
      </commit>

      <commit order="3">
        <message>feat: add cross-platform setup scripts</message>
        <files>
          <file>setup.ps1</file>
          <file>setup.sh</file>
        </files>
        <notes>Automated installation for Windows (PowerShell) and Unix (Bash). Mark setup.sh as executable.</notes>
      </commit>

      <commit order="4">
        <message>docs: update prompt.xml to reflect current implementation</message>
        <files>
          <file>prompt.xml</file>
        </files>
        <notes>Sync VLM model (glm-ocr:bf16), update implementation details, refresh specifications</notes>
      </commit>

      <commit order="5" condition="optional">
        <message>chore: add .prettierrc and .prettierignore for code formatting</message>
        <files>
          <file>.prettierrc.json</file>
          <file>.prettierignore</file>
        </files>
        <notes>Only if user requests Prettier setup for Markdown/XML formatting</notes>
      </commit>
    </commits>

    <push_strategy>
      <validation>
        <step>Test README commands on fresh VM/container for target OS</step>
        <step>Run setup scripts and verify successful installation</step>
        <step>Test extract_usernames.py with sample images (no regressions)</step>
        <step>Check git status for untracked files (__pycache__, *.pyc, test images)</step>
        <step>Review git log --oneline for conventional commit format</step>
      </validation>
      <command>git push origin main</command>
    </push_strategy>
  </commit_strategy>

  <quality_standards>
    <documentation>
      <standard>All commands tested on target platform</standard>
      <standard>No broken links (verify python.org, ollama.com, GitHub URLs)</standard>
      <standard>Code blocks specify language (```bash, ```python, ```powershell)</standard>
      <standard>Provide context before commands (explain what and why)</standard>
    </documentation>

    <code>
      <standard>No changes to extract_usernames.py unless explicitly requested</standard>
      <standard>Setup scripts must fail gracefully with actionable error messages</standard>
      <standard>requirements.txt must not break existing installations</standard>
    </code>

    <user_experience>
      <standard>Installation completes in <15 minutes on typical hardware</standard>
      <standard>First successful extraction within 5 minutes post-install</standard>
      <standard>Error messages guide to solution ("failed because X, try Y")</standard>
    </user_experience>
  </quality_standards>

  <success_criteria>
    <criterion>README enables non-developer to install and run independently</criterion>
    <criterion>Setup scripts reduce manual installation steps by 50%+</criterion>
    <criterion>VLM model choice (glm-ocr:bf16) is justified with clear rationale</criterion>
    <criterion>All commits follow conventional commits format and are atomic</criterion>
    <criterion>Repository passes "fresh clone test" (install and run on clean Ubuntu VM)</criterion>
  </success_criteria>

  <critical_constraints>
    <constraint priority="critical">Do NOT modify crop region constants (TOP_OFFSET=165, CROP_HEIGHT=90, margins) without explicit user request and test images</constraint>
    <constraint priority="critical">Do NOT split extract_usernames.py into multiple files (single-file architecture is intentional for portability)</constraint>
    <constraint priority="critical">Do NOT change VLM_MODEL constant unless user explicitly requests model swap</constraint>
    <constraint priority="high">Maintain backward compatibility (existing verified_usernames.md and needs_review.md files must remain valid)</constraint>
    <constraint priority="high">Preserve EasyOCR-only fallback (--no-vlm must work without Ollama installed)</constraint>
    <constraint priority="medium">Keep documentation accessible to non-technical users (avoid jargon, provide examples)</constraint>
  </critical_constraints>

  <maintenance_notes>
    <note>If glm-ocr:bf16 is deprecated by Ollama, alternatives in order of preference: minicpm-v:8b-2.6-q8_0, qwen2.5vl:7b, olmOCR-2-7B-1025</note>
    <note>If Ollama API changes, fallback to EasyOCR-only mode is already implemented</note>
    <note>Crop region constants assume Instagram's mobile screenshot layout as of Feb 2026, may need adjustment if Instagram redesigns UI</note>
    <note>Preprocessing variants (balanced/aggressive/minimal) are tuned for typical smartphone camera quality, may need retuning for professional cameras or scanners</note>
  </maintenance_notes>
</task_specification>